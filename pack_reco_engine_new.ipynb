{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "pack_reco_engine_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jahid1990/Product-Recommendation-Engine-Using-Deep-Learning/blob/master/pack_reco_engine_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3vWSd7LrDmh",
        "colab_type": "text"
      },
      "source": [
        "# Load required packes and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYEwGGMSrDmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a339c9b-8442-434e-a100-80a94a31d87a"
      },
      "source": [
        "#import cx_Oracle\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, LSTM, Dense,Conv1D,MaxPooling1D,Flatten,Embedding\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras import metrics\n",
        "from sklearn import preprocessing\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqQO26V5rL52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "7bd374bd-726b-4cbc-9a15-af6907734118"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1nOvSucrDot",
        "colab_type": "text"
      },
      "source": [
        "# Saving & loading data to/from PC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cusXzCtqrDov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open(r'training_data_pack_reco_new.pickle', 'wb') as f:\n",
        "#      pickle.dump([x_train,y_train,x_test,y_test], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKctbOhzrDo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(r'/content/drive/My Drive/training_data_pack_reco_new_with_rcg_info.pickle', 'rb') as f:\n",
        "    [x_train,y_train,x_test,y_test]= pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUNy7Kv8rDpe",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f32jJul8rDpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c978d85f-7666-41a2-bb89-db788be9dcee"
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "\n",
        "inputs = Input(shape=(x_train.shape[1],))\n",
        "shared=Dense(1024,activation='relu')(inputs)\n",
        "shared=Dense(1024,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "shared=Dense(512,activation='relu')(shared)\n",
        "\n",
        "pack=Dense(512,activation='relu')(shared)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='relu')(pack)\n",
        "pack=Dense(512,activation='tanh')(pack)\n",
        "pack=Dense(16,activation='softmax',name='pack')(pack)\n",
        "\n",
        "day=Dense(512,activation='relu')(shared)\n",
        "day=Dense(512,activation='relu')(day)\n",
        "day=Dense(512,activation='relu')(day)\n",
        "day=Dense(512,activation='relu')(day)\n",
        "day=Dense(256,activation='relu')(day)\n",
        "day=Dense(128,activation='tanh')(day)\n",
        "day=Dense(32,activation='softmax',name='day')(day)\n",
        "\n",
        "\n",
        "model=Model(inputs=inputs,outputs=[pack,day])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 126)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 1024)         130048      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 1024)         1049600     dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 512)          524800      dense_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 512)          262656      dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 512)          262656      dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 512)          262656      dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 512)          262656      dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 512)          262656      dense_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 512)          262656      dense_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 512)          262656      dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 512)          262656      dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 512)          262656      dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 512)          262656      dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 512)          262656      dense_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 512)          262656      dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 512)          262656      dense_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 512)          262656      dense_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 512)          262656      dense_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 512)          262656      dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 512)          262656      dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 512)          262656      dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 512)          262656      dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 512)          262656      dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 512)          262656      dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (None, 512)          262656      dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 512)          262656      dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 256)          131328      dense_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 512)          262656      dense_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (None, 128)          32896       dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pack (Dense)                    (None, 16)           8208        dense_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "day (Dense)                     (None, 32)           4128        dense_53[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 8,184,752\n",
            "Trainable params: 8,184,752\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDNqoXPErDpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "adm=optimizers.Adam( lr=0.001,beta_1=0.9, beta_2=0.999, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn_GH05irDpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "checkpoint = ModelCheckpoint('/content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5', \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min', \n",
        "                             period=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "B0_V-4KDrDqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9e01d25-1730-4d41-c849-48c4aae8d1da"
      },
      "source": [
        "#[x_test,[y_test[:,:16],y_test[:,16:]]]\n",
        "model.compile(loss=['categorical_crossentropy','categorical_crossentropy'],optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit(x_train, [y_train[:,:16],y_train[:,16:]] ,epochs=100,validation_split=.2,verbose=1,batch_size=512,callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1794568 samples, validate on 448642 samples\n",
            "Epoch 1/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 4.0543 - pack_loss: 1.6483 - day_loss: 2.4058 - pack_accuracy: 0.4848 - day_accuracy: 0.3792 - val_loss: 4.0046 - val_pack_loss: 1.6141 - val_day_loss: 2.3905 - val_pack_accuracy: 0.4930 - val_day_accuracy: 0.3839\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.00463, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 2/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.7799 - pack_loss: 1.4554 - day_loss: 2.3247 - pack_accuracy: 0.5401 - day_accuracy: 0.3945 - val_loss: 3.7346 - val_pack_loss: 1.4460 - val_day_loss: 2.2885 - val_pack_accuracy: 0.5466 - val_day_accuracy: 0.4021\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.00463 to 3.73463, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 3/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.5428 - pack_loss: 1.2764 - day_loss: 2.2666 - pack_accuracy: 0.5940 - day_accuracy: 0.4019 - val_loss: 3.6682 - val_pack_loss: 1.3407 - val_day_loss: 2.3274 - val_pack_accuracy: 0.5668 - val_day_accuracy: 0.3897\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.73463 to 3.66818, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 4/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.4857 - pack_loss: 1.2352 - day_loss: 2.2499 - pack_accuracy: 0.6086 - day_accuracy: 0.4052 - val_loss: 3.4901 - val_pack_loss: 1.2339 - val_day_loss: 2.2563 - val_pack_accuracy: 0.6035 - val_day_accuracy: 0.4019\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.66818 to 3.49010, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 5/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.3812 - pack_loss: 1.1656 - day_loss: 2.2153 - pack_accuracy: 0.6305 - day_accuracy: 0.4135 - val_loss: 3.4016 - val_pack_loss: 1.1638 - val_day_loss: 2.2378 - val_pack_accuracy: 0.6330 - val_day_accuracy: 0.4067\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.49010 to 3.40157, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 6/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.3528 - pack_loss: 1.1506 - day_loss: 2.2019 - pack_accuracy: 0.6351 - day_accuracy: 0.4176 - val_loss: 3.4169 - val_pack_loss: 1.1557 - val_day_loss: 2.2613 - val_pack_accuracy: 0.6326 - val_day_accuracy: 0.4063\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 3.40157\n",
            "Epoch 7/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.3449 - pack_loss: 1.1482 - day_loss: 2.1968 - pack_accuracy: 0.6363 - day_accuracy: 0.4193 - val_loss: 3.3819 - val_pack_loss: 1.1691 - val_day_loss: 2.2127 - val_pack_accuracy: 0.6340 - val_day_accuracy: 0.4159\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.40157 to 3.38194, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 8/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.3221 - pack_loss: 1.1381 - day_loss: 2.1838 - pack_accuracy: 0.6390 - day_accuracy: 0.4234 - val_loss: 3.3739 - val_pack_loss: 1.1561 - val_day_loss: 2.2178 - val_pack_accuracy: 0.6310 - val_day_accuracy: 0.4196\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.38194 to 3.37387, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 9/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.3164 - pack_loss: 1.1363 - day_loss: 2.1802 - pack_accuracy: 0.6395 - day_accuracy: 0.4253 - val_loss: 3.3569 - val_pack_loss: 1.1447 - val_day_loss: 2.2122 - val_pack_accuracy: 0.6343 - val_day_accuracy: 0.4156\n",
            "\n",
            "Epoch 00009: val_loss improved from 3.37387 to 3.35688, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 10/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.3088 - pack_loss: 1.1354 - day_loss: 2.1732 - pack_accuracy: 0.6392 - day_accuracy: 0.4282 - val_loss: 3.3959 - val_pack_loss: 1.1830 - val_day_loss: 2.2129 - val_pack_accuracy: 0.6258 - val_day_accuracy: 0.4262\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3.35688\n",
            "Epoch 11/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.3196 - pack_loss: 1.1433 - day_loss: 2.1760 - pack_accuracy: 0.6375 - day_accuracy: 0.4273 - val_loss: 3.3174 - val_pack_loss: 1.1402 - val_day_loss: 2.1772 - val_pack_accuracy: 0.6378 - val_day_accuracy: 0.4301\n",
            "\n",
            "Epoch 00011: val_loss improved from 3.35688 to 3.31741, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 12/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2990 - pack_loss: 1.1307 - day_loss: 2.1683 - pack_accuracy: 0.6406 - day_accuracy: 0.4303 - val_loss: 3.3627 - val_pack_loss: 1.1462 - val_day_loss: 2.2165 - val_pack_accuracy: 0.6336 - val_day_accuracy: 0.4182\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.31741\n",
            "Epoch 13/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2922 - pack_loss: 1.1277 - day_loss: 2.1642 - pack_accuracy: 0.6414 - day_accuracy: 0.4312 - val_loss: 3.3570 - val_pack_loss: 1.1526 - val_day_loss: 2.2045 - val_pack_accuracy: 0.6278 - val_day_accuracy: 0.4221\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.31741\n",
            "Epoch 14/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2901 - pack_loss: 1.1274 - day_loss: 2.1628 - pack_accuracy: 0.6423 - day_accuracy: 0.4315 - val_loss: 3.3115 - val_pack_loss: 1.1440 - val_day_loss: 2.1676 - val_pack_accuracy: 0.6364 - val_day_accuracy: 0.4296\n",
            "\n",
            "Epoch 00014: val_loss improved from 3.31741 to 3.31154, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 15/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.2918 - pack_loss: 1.1289 - day_loss: 2.1631 - pack_accuracy: 0.6404 - day_accuracy: 0.4317 - val_loss: 3.3965 - val_pack_loss: 1.1714 - val_day_loss: 2.2251 - val_pack_accuracy: 0.6232 - val_day_accuracy: 0.4139\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.31154\n",
            "Epoch 16/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.3319 - pack_loss: 1.1530 - day_loss: 2.1787 - pack_accuracy: 0.6347 - day_accuracy: 0.4267 - val_loss: 3.3493 - val_pack_loss: 1.1682 - val_day_loss: 2.1810 - val_pack_accuracy: 0.6273 - val_day_accuracy: 0.4240\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.31154\n",
            "Epoch 17/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.3347 - pack_loss: 1.1545 - day_loss: 2.1806 - pack_accuracy: 0.6347 - day_accuracy: 0.4249 - val_loss: 3.3318 - val_pack_loss: 1.1472 - val_day_loss: 2.1846 - val_pack_accuracy: 0.6371 - val_day_accuracy: 0.4213\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.31154\n",
            "Epoch 18/100\n",
            "1794568/1794568 [==============================] - 69s 39us/step - loss: 3.3052 - pack_loss: 1.1334 - day_loss: 2.1721 - pack_accuracy: 0.6396 - day_accuracy: 0.4268 - val_loss: 3.3181 - val_pack_loss: 1.1431 - val_day_loss: 2.1750 - val_pack_accuracy: 0.6380 - val_day_accuracy: 0.4271\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.31154\n",
            "Epoch 19/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.3088 - pack_loss: 1.1372 - day_loss: 2.1713 - pack_accuracy: 0.6384 - day_accuracy: 0.4287 - val_loss: 3.3389 - val_pack_loss: 1.1471 - val_day_loss: 2.1918 - val_pack_accuracy: 0.6345 - val_day_accuracy: 0.4209\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.31154\n",
            "Epoch 20/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.2995 - pack_loss: 1.1329 - day_loss: 2.1668 - pack_accuracy: 0.6398 - day_accuracy: 0.4303 - val_loss: 3.3126 - val_pack_loss: 1.1380 - val_day_loss: 2.1747 - val_pack_accuracy: 0.6357 - val_day_accuracy: 0.4295\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.31154\n",
            "Epoch 21/100\n",
            "1794568/1794568 [==============================] - 73s 41us/step - loss: 3.3154 - pack_loss: 1.1414 - day_loss: 2.1739 - pack_accuracy: 0.6366 - day_accuracy: 0.4281 - val_loss: 3.3053 - val_pack_loss: 1.1382 - val_day_loss: 2.1672 - val_pack_accuracy: 0.6383 - val_day_accuracy: 0.4284\n",
            "\n",
            "Epoch 00021: val_loss improved from 3.31154 to 3.30533, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 22/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.3067 - pack_loss: 1.1364 - day_loss: 2.1702 - pack_accuracy: 0.6394 - day_accuracy: 0.4291 - val_loss: 3.2899 - val_pack_loss: 1.1277 - val_day_loss: 2.1623 - val_pack_accuracy: 0.6416 - val_day_accuracy: 0.4324\n",
            "\n",
            "Epoch 00022: val_loss improved from 3.30533 to 3.28988, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 23/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.2948 - pack_loss: 1.1304 - day_loss: 2.1643 - pack_accuracy: 0.6397 - day_accuracy: 0.4316 - val_loss: 3.2992 - val_pack_loss: 1.1299 - val_day_loss: 2.1693 - val_pack_accuracy: 0.6393 - val_day_accuracy: 0.4305\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.28988\n",
            "Epoch 24/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.2759 - pack_loss: 1.1209 - day_loss: 2.1550 - pack_accuracy: 0.6418 - day_accuracy: 0.4343 - val_loss: 3.2821 - val_pack_loss: 1.1269 - val_day_loss: 2.1553 - val_pack_accuracy: 0.6417 - val_day_accuracy: 0.4348\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.28988 to 3.28210, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 25/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.2870 - pack_loss: 1.1260 - day_loss: 2.1614 - pack_accuracy: 0.6410 - day_accuracy: 0.4322 - val_loss: 3.2973 - val_pack_loss: 1.1354 - val_day_loss: 2.1619 - val_pack_accuracy: 0.6379 - val_day_accuracy: 0.4331\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 3.28210\n",
            "Epoch 26/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.2955 - pack_loss: 1.1307 - day_loss: 2.1649 - pack_accuracy: 0.6393 - day_accuracy: 0.4317 - val_loss: 3.2859 - val_pack_loss: 1.1284 - val_day_loss: 2.1575 - val_pack_accuracy: 0.6413 - val_day_accuracy: 0.4330\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.28210\n",
            "Epoch 27/100\n",
            "1794568/1794568 [==============================] - 72s 40us/step - loss: 3.2970 - pack_loss: 1.1327 - day_loss: 2.1642 - pack_accuracy: 0.6391 - day_accuracy: 0.4313 - val_loss: 3.3025 - val_pack_loss: 1.1286 - val_day_loss: 2.1740 - val_pack_accuracy: 0.6396 - val_day_accuracy: 0.4274\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 3.28210\n",
            "Epoch 28/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2890 - pack_loss: 1.1263 - day_loss: 2.1628 - pack_accuracy: 0.6407 - day_accuracy: 0.4324 - val_loss: 3.2910 - val_pack_loss: 1.1276 - val_day_loss: 2.1634 - val_pack_accuracy: 0.6398 - val_day_accuracy: 0.4321\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 3.28210\n",
            "Epoch 29/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2901 - pack_loss: 1.1267 - day_loss: 2.1633 - pack_accuracy: 0.6399 - day_accuracy: 0.4319 - val_loss: 3.3064 - val_pack_loss: 1.1392 - val_day_loss: 2.1672 - val_pack_accuracy: 0.6357 - val_day_accuracy: 0.4330\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 3.28210\n",
            "Epoch 30/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.3075 - pack_loss: 1.1407 - day_loss: 2.1669 - pack_accuracy: 0.6365 - day_accuracy: 0.4312 - val_loss: 3.4393 - val_pack_loss: 1.2166 - val_day_loss: 2.2228 - val_pack_accuracy: 0.6167 - val_day_accuracy: 0.4210\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 3.28210\n",
            "Epoch 31/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.3344 - pack_loss: 1.1575 - day_loss: 2.1768 - pack_accuracy: 0.6333 - day_accuracy: 0.4273 - val_loss: 3.3146 - val_pack_loss: 1.1452 - val_day_loss: 2.1695 - val_pack_accuracy: 0.6358 - val_day_accuracy: 0.4290\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 3.28210\n",
            "Epoch 32/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2924 - pack_loss: 1.1324 - day_loss: 2.1599 - pack_accuracy: 0.6396 - day_accuracy: 0.4329 - val_loss: 3.3697 - val_pack_loss: 1.1901 - val_day_loss: 2.1795 - val_pack_accuracy: 0.6184 - val_day_accuracy: 0.4237\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.28210\n",
            "Epoch 33/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2902 - pack_loss: 1.1342 - day_loss: 2.1565 - pack_accuracy: 0.6394 - day_accuracy: 0.4353 - val_loss: 3.2976 - val_pack_loss: 1.1400 - val_day_loss: 2.1577 - val_pack_accuracy: 0.6394 - val_day_accuracy: 0.4361\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.28210\n",
            "Epoch 34/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2786 - pack_loss: 1.1265 - day_loss: 2.1523 - pack_accuracy: 0.6415 - day_accuracy: 0.4361 - val_loss: 3.2952 - val_pack_loss: 1.1398 - val_day_loss: 2.1554 - val_pack_accuracy: 0.6373 - val_day_accuracy: 0.4352\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.28210\n",
            "Epoch 35/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.3055 - pack_loss: 1.1401 - day_loss: 2.1650 - pack_accuracy: 0.6374 - day_accuracy: 0.4326 - val_loss: 3.2848 - val_pack_loss: 1.1240 - val_day_loss: 2.1608 - val_pack_accuracy: 0.6406 - val_day_accuracy: 0.4347\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.28210\n",
            "Epoch 36/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2790 - pack_loss: 1.1272 - day_loss: 2.1516 - pack_accuracy: 0.6413 - day_accuracy: 0.4368 - val_loss: 3.3026 - val_pack_loss: 1.1446 - val_day_loss: 2.1581 - val_pack_accuracy: 0.6382 - val_day_accuracy: 0.4333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 3.28210\n",
            "Epoch 37/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2823 - pack_loss: 1.1286 - day_loss: 2.1538 - pack_accuracy: 0.6414 - day_accuracy: 0.4359 - val_loss: 3.2720 - val_pack_loss: 1.1205 - val_day_loss: 2.1516 - val_pack_accuracy: 0.6431 - val_day_accuracy: 0.4374\n",
            "\n",
            "Epoch 00037: val_loss improved from 3.28210 to 3.27203, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 38/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2933 - pack_loss: 1.1334 - day_loss: 2.1599 - pack_accuracy: 0.6394 - day_accuracy: 0.4346 - val_loss: 3.3997 - val_pack_loss: 1.1933 - val_day_loss: 2.2063 - val_pack_accuracy: 0.6215 - val_day_accuracy: 0.4287\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.27203\n",
            "Epoch 39/100\n",
            "1794568/1794568 [==============================] - 69s 39us/step - loss: 3.3087 - pack_loss: 1.1472 - day_loss: 2.1617 - pack_accuracy: 0.6361 - day_accuracy: 0.4342 - val_loss: 3.4519 - val_pack_loss: 1.2603 - val_day_loss: 2.1915 - val_pack_accuracy: 0.6076 - val_day_accuracy: 0.4282\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 3.27203\n",
            "Epoch 40/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.3272 - pack_loss: 1.1515 - day_loss: 2.1754 - pack_accuracy: 0.6362 - day_accuracy: 0.4294 - val_loss: 3.3321 - val_pack_loss: 1.1603 - val_day_loss: 2.1717 - val_pack_accuracy: 0.6331 - val_day_accuracy: 0.4303\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 3.27203\n",
            "Epoch 41/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2852 - pack_loss: 1.1316 - day_loss: 2.1536 - pack_accuracy: 0.6412 - day_accuracy: 0.4361 - val_loss: 3.3031 - val_pack_loss: 1.1393 - val_day_loss: 2.1638 - val_pack_accuracy: 0.6366 - val_day_accuracy: 0.4341\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 3.27203\n",
            "Epoch 42/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2771 - pack_loss: 1.1262 - day_loss: 2.1515 - pack_accuracy: 0.6426 - day_accuracy: 0.4365 - val_loss: 3.3239 - val_pack_loss: 1.1712 - val_day_loss: 2.1527 - val_pack_accuracy: 0.6287 - val_day_accuracy: 0.4372\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 3.27203\n",
            "Epoch 43/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2729 - pack_loss: 1.1232 - day_loss: 2.1496 - pack_accuracy: 0.6433 - day_accuracy: 0.4365 - val_loss: 3.3022 - val_pack_loss: 1.1484 - val_day_loss: 2.1540 - val_pack_accuracy: 0.6386 - val_day_accuracy: 0.4354\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 3.27203\n",
            "Epoch 44/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2844 - pack_loss: 1.1279 - day_loss: 2.1569 - pack_accuracy: 0.6417 - day_accuracy: 0.4346 - val_loss: 3.2763 - val_pack_loss: 1.1318 - val_day_loss: 2.1445 - val_pack_accuracy: 0.6424 - val_day_accuracy: 0.4386\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 3.27203\n",
            "Epoch 45/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2853 - pack_loss: 1.1312 - day_loss: 2.1546 - pack_accuracy: 0.6411 - day_accuracy: 0.4351 - val_loss: 3.2669 - val_pack_loss: 1.1210 - val_day_loss: 2.1460 - val_pack_accuracy: 0.6438 - val_day_accuracy: 0.4380\n",
            "\n",
            "Epoch 00045: val_loss improved from 3.27203 to 3.26693, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 46/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2582 - pack_loss: 1.1166 - day_loss: 2.1415 - pack_accuracy: 0.6446 - day_accuracy: 0.4394 - val_loss: 3.2717 - val_pack_loss: 1.1216 - val_day_loss: 2.1501 - val_pack_accuracy: 0.6442 - val_day_accuracy: 0.4388\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 3.26693\n",
            "Epoch 47/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2641 - pack_loss: 1.1190 - day_loss: 2.1458 - pack_accuracy: 0.6443 - day_accuracy: 0.4376 - val_loss: 3.3184 - val_pack_loss: 1.1711 - val_day_loss: 2.1473 - val_pack_accuracy: 0.6314 - val_day_accuracy: 0.4374\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 3.26693\n",
            "Epoch 48/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2660 - pack_loss: 1.1195 - day_loss: 2.1464 - pack_accuracy: 0.6434 - day_accuracy: 0.4370 - val_loss: 3.3126 - val_pack_loss: 1.1510 - val_day_loss: 2.1616 - val_pack_accuracy: 0.6367 - val_day_accuracy: 0.4316\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 3.26693\n",
            "Epoch 49/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2610 - pack_loss: 1.1165 - day_loss: 2.1449 - pack_accuracy: 0.6444 - day_accuracy: 0.4385 - val_loss: 3.3080 - val_pack_loss: 1.1368 - val_day_loss: 2.1713 - val_pack_accuracy: 0.6395 - val_day_accuracy: 0.4335\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 3.26693\n",
            "Epoch 50/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2564 - pack_loss: 1.1153 - day_loss: 2.1414 - pack_accuracy: 0.6447 - day_accuracy: 0.4397 - val_loss: 3.2835 - val_pack_loss: 1.1321 - val_day_loss: 2.1515 - val_pack_accuracy: 0.6435 - val_day_accuracy: 0.4325\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 3.26693\n",
            "Epoch 51/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2557 - pack_loss: 1.1137 - day_loss: 2.1422 - pack_accuracy: 0.6453 - day_accuracy: 0.4388 - val_loss: 3.2609 - val_pack_loss: 1.1158 - val_day_loss: 2.1452 - val_pack_accuracy: 0.6444 - val_day_accuracy: 0.4376\n",
            "\n",
            "Epoch 00051: val_loss improved from 3.26693 to 3.26092, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 52/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2483 - pack_loss: 1.1101 - day_loss: 2.1383 - pack_accuracy: 0.6462 - day_accuracy: 0.4398 - val_loss: 3.2671 - val_pack_loss: 1.1241 - val_day_loss: 2.1431 - val_pack_accuracy: 0.6440 - val_day_accuracy: 0.4371\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 3.26092\n",
            "Epoch 53/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2563 - pack_loss: 1.1157 - day_loss: 2.1405 - pack_accuracy: 0.6447 - day_accuracy: 0.4395 - val_loss: 3.2846 - val_pack_loss: 1.1338 - val_day_loss: 2.1508 - val_pack_accuracy: 0.6397 - val_day_accuracy: 0.4359\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 3.26092\n",
            "Epoch 54/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2496 - pack_loss: 1.1113 - day_loss: 2.1380 - pack_accuracy: 0.6452 - day_accuracy: 0.4403 - val_loss: 3.2633 - val_pack_loss: 1.1223 - val_day_loss: 2.1410 - val_pack_accuracy: 0.6438 - val_day_accuracy: 0.4402\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 3.26092\n",
            "Epoch 55/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2489 - pack_loss: 1.1112 - day_loss: 2.1380 - pack_accuracy: 0.6458 - day_accuracy: 0.4398 - val_loss: 3.3181 - val_pack_loss: 1.1545 - val_day_loss: 2.1637 - val_pack_accuracy: 0.6331 - val_day_accuracy: 0.4303\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 3.26092\n",
            "Epoch 56/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2627 - pack_loss: 1.1167 - day_loss: 2.1461 - pack_accuracy: 0.6438 - day_accuracy: 0.4370 - val_loss: 3.2594 - val_pack_loss: 1.1142 - val_day_loss: 2.1453 - val_pack_accuracy: 0.6448 - val_day_accuracy: 0.4359\n",
            "\n",
            "Epoch 00056: val_loss improved from 3.26092 to 3.25936, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 57/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2557 - pack_loss: 1.1158 - day_loss: 2.1398 - pack_accuracy: 0.6438 - day_accuracy: 0.4392 - val_loss: 3.2701 - val_pack_loss: 1.1229 - val_day_loss: 2.1472 - val_pack_accuracy: 0.6431 - val_day_accuracy: 0.4377\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 3.25936\n",
            "Epoch 58/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2520 - pack_loss: 1.1138 - day_loss: 2.1379 - pack_accuracy: 0.6448 - day_accuracy: 0.4393 - val_loss: 3.2613 - val_pack_loss: 1.1196 - val_day_loss: 2.1418 - val_pack_accuracy: 0.6431 - val_day_accuracy: 0.4396\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 3.25936\n",
            "Epoch 59/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2526 - pack_loss: 1.1130 - day_loss: 2.1394 - pack_accuracy: 0.6437 - day_accuracy: 0.4397 - val_loss: 3.2557 - val_pack_loss: 1.1166 - val_day_loss: 2.1391 - val_pack_accuracy: 0.6447 - val_day_accuracy: 0.4399\n",
            "\n",
            "Epoch 00059: val_loss improved from 3.25936 to 3.25568, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 60/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2510 - pack_loss: 1.1110 - day_loss: 2.1401 - pack_accuracy: 0.6453 - day_accuracy: 0.4394 - val_loss: 3.2543 - val_pack_loss: 1.1169 - val_day_loss: 2.1373 - val_pack_accuracy: 0.6444 - val_day_accuracy: 0.4399\n",
            "\n",
            "Epoch 00060: val_loss improved from 3.25568 to 3.25429, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 61/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2485 - pack_loss: 1.1086 - day_loss: 2.1401 - pack_accuracy: 0.6459 - day_accuracy: 0.4391 - val_loss: 3.2664 - val_pack_loss: 1.1190 - val_day_loss: 2.1475 - val_pack_accuracy: 0.6433 - val_day_accuracy: 0.4381\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 3.25429\n",
            "Epoch 62/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2394 - pack_loss: 1.1055 - day_loss: 2.1341 - pack_accuracy: 0.6468 - day_accuracy: 0.4409 - val_loss: 3.2691 - val_pack_loss: 1.1249 - val_day_loss: 2.1443 - val_pack_accuracy: 0.6417 - val_day_accuracy: 0.4392\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 3.25429\n",
            "Epoch 63/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2425 - pack_loss: 1.1077 - day_loss: 2.1348 - pack_accuracy: 0.6464 - day_accuracy: 0.4411 - val_loss: 3.2480 - val_pack_loss: 1.1076 - val_day_loss: 2.1404 - val_pack_accuracy: 0.6472 - val_day_accuracy: 0.4396\n",
            "\n",
            "Epoch 00063: val_loss improved from 3.25429 to 3.24796, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 64/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2331 - pack_loss: 1.1022 - day_loss: 2.1308 - pack_accuracy: 0.6478 - day_accuracy: 0.4421 - val_loss: 3.2634 - val_pack_loss: 1.1180 - val_day_loss: 2.1454 - val_pack_accuracy: 0.6444 - val_day_accuracy: 0.4383\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 3.24796\n",
            "Epoch 65/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2330 - pack_loss: 1.1028 - day_loss: 2.1300 - pack_accuracy: 0.6477 - day_accuracy: 0.4422 - val_loss: 3.2813 - val_pack_loss: 1.1290 - val_day_loss: 2.1523 - val_pack_accuracy: 0.6415 - val_day_accuracy: 0.4336\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 3.24796\n",
            "Epoch 66/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2303 - pack_loss: 1.1012 - day_loss: 2.1290 - pack_accuracy: 0.6479 - day_accuracy: 0.4425 - val_loss: 3.2480 - val_pack_loss: 1.1064 - val_day_loss: 2.1416 - val_pack_accuracy: 0.6477 - val_day_accuracy: 0.4372\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 3.24796\n",
            "Epoch 67/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2475 - pack_loss: 1.1101 - day_loss: 2.1371 - pack_accuracy: 0.6458 - day_accuracy: 0.4411 - val_loss: 3.3125 - val_pack_loss: 1.1492 - val_day_loss: 2.1634 - val_pack_accuracy: 0.6341 - val_day_accuracy: 0.4376\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 3.24796\n",
            "Epoch 68/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2451 - pack_loss: 1.1101 - day_loss: 2.1351 - pack_accuracy: 0.6453 - day_accuracy: 0.4416 - val_loss: 3.2530 - val_pack_loss: 1.1071 - val_day_loss: 2.1460 - val_pack_accuracy: 0.6476 - val_day_accuracy: 0.4386\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 3.24796\n",
            "Epoch 69/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2286 - pack_loss: 1.0995 - day_loss: 2.1285 - pack_accuracy: 0.6482 - day_accuracy: 0.4424 - val_loss: 3.2345 - val_pack_loss: 1.1029 - val_day_loss: 2.1316 - val_pack_accuracy: 0.6470 - val_day_accuracy: 0.4418\n",
            "\n",
            "Epoch 00069: val_loss improved from 3.24796 to 3.23448, saving model to /content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5\n",
            "Epoch 70/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2352 - pack_loss: 1.1031 - day_loss: 2.1324 - pack_accuracy: 0.6472 - day_accuracy: 0.4416 - val_loss: 3.3045 - val_pack_loss: 1.1333 - val_day_loss: 2.1712 - val_pack_accuracy: 0.6373 - val_day_accuracy: 0.4315\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 3.23448\n",
            "Epoch 71/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2362 - pack_loss: 1.1033 - day_loss: 2.1325 - pack_accuracy: 0.6474 - day_accuracy: 0.4414 - val_loss: 3.2540 - val_pack_loss: 1.1114 - val_day_loss: 2.1427 - val_pack_accuracy: 0.6459 - val_day_accuracy: 0.4377\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 3.23448\n",
            "Epoch 72/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2650 - pack_loss: 1.1217 - day_loss: 2.1435 - pack_accuracy: 0.6419 - day_accuracy: 0.4389 - val_loss: 3.2755 - val_pack_loss: 1.1342 - val_day_loss: 2.1414 - val_pack_accuracy: 0.6400 - val_day_accuracy: 0.4388\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 3.23448\n",
            "Epoch 73/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2362 - pack_loss: 1.1073 - day_loss: 2.1288 - pack_accuracy: 0.6467 - day_accuracy: 0.4427 - val_loss: 3.2645 - val_pack_loss: 1.1227 - val_day_loss: 2.1419 - val_pack_accuracy: 0.6443 - val_day_accuracy: 0.4407\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 3.23448\n",
            "Epoch 74/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2336 - pack_loss: 1.1054 - day_loss: 2.1282 - pack_accuracy: 0.6469 - day_accuracy: 0.4424 - val_loss: 3.2675 - val_pack_loss: 1.1174 - val_day_loss: 2.1501 - val_pack_accuracy: 0.6434 - val_day_accuracy: 0.4370\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 3.23448\n",
            "Epoch 75/100\n",
            "1794568/1794568 [==============================] - 69s 39us/step - loss: 3.2306 - pack_loss: 1.1041 - day_loss: 2.1266 - pack_accuracy: 0.6481 - day_accuracy: 0.4433 - val_loss: 3.2506 - val_pack_loss: 1.1108 - val_day_loss: 2.1398 - val_pack_accuracy: 0.6474 - val_day_accuracy: 0.4380\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 3.23448\n",
            "Epoch 76/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2294 - pack_loss: 1.1044 - day_loss: 2.1252 - pack_accuracy: 0.6480 - day_accuracy: 0.4435 - val_loss: 3.2534 - val_pack_loss: 1.1200 - val_day_loss: 2.1334 - val_pack_accuracy: 0.6441 - val_day_accuracy: 0.4432\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 3.23448\n",
            "Epoch 77/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2281 - pack_loss: 1.1026 - day_loss: 2.1252 - pack_accuracy: 0.6482 - day_accuracy: 0.4433 - val_loss: 3.2537 - val_pack_loss: 1.1136 - val_day_loss: 2.1400 - val_pack_accuracy: 0.6439 - val_day_accuracy: 0.4400\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 3.23448\n",
            "Epoch 78/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2341 - pack_loss: 1.1053 - day_loss: 2.1290 - pack_accuracy: 0.6475 - day_accuracy: 0.4426 - val_loss: 3.2518 - val_pack_loss: 1.1173 - val_day_loss: 2.1344 - val_pack_accuracy: 0.6432 - val_day_accuracy: 0.4412\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 3.23448\n",
            "Epoch 79/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2269 - pack_loss: 1.1018 - day_loss: 2.1252 - pack_accuracy: 0.6481 - day_accuracy: 0.4433 - val_loss: 3.2649 - val_pack_loss: 1.1304 - val_day_loss: 2.1345 - val_pack_accuracy: 0.6396 - val_day_accuracy: 0.4419\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 3.23448\n",
            "Epoch 80/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2275 - pack_loss: 1.1020 - day_loss: 2.1257 - pack_accuracy: 0.6482 - day_accuracy: 0.4435 - val_loss: 3.2438 - val_pack_loss: 1.1121 - val_day_loss: 2.1317 - val_pack_accuracy: 0.6450 - val_day_accuracy: 0.4420\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 3.23448\n",
            "Epoch 81/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2408 - pack_loss: 1.1085 - day_loss: 2.1325 - pack_accuracy: 0.6460 - day_accuracy: 0.4422 - val_loss: 3.2604 - val_pack_loss: 1.1165 - val_day_loss: 2.1440 - val_pack_accuracy: 0.6445 - val_day_accuracy: 0.4378\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 3.23448\n",
            "Epoch 82/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2567 - pack_loss: 1.1181 - day_loss: 2.1389 - pack_accuracy: 0.6441 - day_accuracy: 0.4411 - val_loss: 3.2572 - val_pack_loss: 1.1165 - val_day_loss: 2.1408 - val_pack_accuracy: 0.6440 - val_day_accuracy: 0.4401\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 3.23448\n",
            "Epoch 83/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2530 - pack_loss: 1.1157 - day_loss: 2.1370 - pack_accuracy: 0.6443 - day_accuracy: 0.4416 - val_loss: 3.2508 - val_pack_loss: 1.1150 - val_day_loss: 2.1358 - val_pack_accuracy: 0.6455 - val_day_accuracy: 0.4421\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 3.23448\n",
            "Epoch 84/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.2532 - pack_loss: 1.1176 - day_loss: 2.1361 - pack_accuracy: 0.6440 - day_accuracy: 0.4409 - val_loss: 3.2707 - val_pack_loss: 1.1240 - val_day_loss: 2.1467 - val_pack_accuracy: 0.6433 - val_day_accuracy: 0.4379\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 3.23448\n",
            "Epoch 85/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2904 - pack_loss: 1.1402 - day_loss: 2.1501 - pack_accuracy: 0.6379 - day_accuracy: 0.4376 - val_loss: 3.2821 - val_pack_loss: 1.1350 - val_day_loss: 2.1472 - val_pack_accuracy: 0.6391 - val_day_accuracy: 0.4375\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 3.23448\n",
            "Epoch 86/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2705 - pack_loss: 1.1304 - day_loss: 2.1406 - pack_accuracy: 0.6417 - day_accuracy: 0.4400 - val_loss: 3.4213 - val_pack_loss: 1.2548 - val_day_loss: 2.1666 - val_pack_accuracy: 0.5890 - val_day_accuracy: 0.4321\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 3.23448\n",
            "Epoch 87/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2757 - pack_loss: 1.1335 - day_loss: 2.1421 - pack_accuracy: 0.6398 - day_accuracy: 0.4392 - val_loss: 3.2954 - val_pack_loss: 1.1457 - val_day_loss: 2.1497 - val_pack_accuracy: 0.6356 - val_day_accuracy: 0.4354\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 3.23448\n",
            "Epoch 88/100\n",
            "1794568/1794568 [==============================] - 69s 39us/step - loss: 3.3226 - pack_loss: 1.1606 - day_loss: 2.1621 - pack_accuracy: 0.6323 - day_accuracy: 0.4353 - val_loss: 3.3599 - val_pack_loss: 1.1757 - val_day_loss: 2.1843 - val_pack_accuracy: 0.6276 - val_day_accuracy: 0.4297\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 3.23448\n",
            "Epoch 89/100\n",
            "1794568/1794568 [==============================] - 69s 38us/step - loss: 3.3221 - pack_loss: 1.1561 - day_loss: 2.1659 - pack_accuracy: 0.6339 - day_accuracy: 0.4340 - val_loss: 3.3267 - val_pack_loss: 1.1691 - val_day_loss: 2.1576 - val_pack_accuracy: 0.6291 - val_day_accuracy: 0.4341\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 3.23448\n",
            "Epoch 90/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2821 - pack_loss: 1.1358 - day_loss: 2.1462 - pack_accuracy: 0.6387 - day_accuracy: 0.4378 - val_loss: 3.3171 - val_pack_loss: 1.1647 - val_day_loss: 2.1524 - val_pack_accuracy: 0.6316 - val_day_accuracy: 0.4376\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 3.23448\n",
            "Epoch 91/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2673 - pack_loss: 1.1269 - day_loss: 2.1406 - pack_accuracy: 0.6406 - day_accuracy: 0.4397 - val_loss: 3.2903 - val_pack_loss: 1.1350 - val_day_loss: 2.1553 - val_pack_accuracy: 0.6396 - val_day_accuracy: 0.4337\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 3.23448\n",
            "Epoch 92/100\n",
            "1794568/1794568 [==============================] - 68s 38us/step - loss: 3.2902 - pack_loss: 1.1374 - day_loss: 2.1523 - pack_accuracy: 0.6384 - day_accuracy: 0.4371 - val_loss: 3.3081 - val_pack_loss: 1.1477 - val_day_loss: 2.1605 - val_pack_accuracy: 0.6352 - val_day_accuracy: 0.4337\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 3.23448\n",
            "Epoch 93/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.2940 - pack_loss: 1.1464 - day_loss: 2.1474 - pack_accuracy: 0.6365 - day_accuracy: 0.4377 - val_loss: 3.2809 - val_pack_loss: 1.1363 - val_day_loss: 2.1447 - val_pack_accuracy: 0.6406 - val_day_accuracy: 0.4389\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 3.23448\n",
            "Epoch 94/100\n",
            "1794568/1794568 [==============================] - 69s 39us/step - loss: 3.2618 - pack_loss: 1.1232 - day_loss: 2.1387 - pack_accuracy: 0.6431 - day_accuracy: 0.4401 - val_loss: 3.2757 - val_pack_loss: 1.1361 - val_day_loss: 2.1397 - val_pack_accuracy: 0.6391 - val_day_accuracy: 0.4395\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 3.23448\n",
            "Epoch 95/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.2562 - pack_loss: 1.1204 - day_loss: 2.1362 - pack_accuracy: 0.6434 - day_accuracy: 0.4409 - val_loss: 3.2970 - val_pack_loss: 1.1460 - val_day_loss: 2.1510 - val_pack_accuracy: 0.6347 - val_day_accuracy: 0.4387\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 3.23448\n",
            "Epoch 96/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.3122 - pack_loss: 1.1471 - day_loss: 2.1652 - pack_accuracy: 0.6352 - day_accuracy: 0.4342 - val_loss: 3.3147 - val_pack_loss: 1.1433 - val_day_loss: 2.1714 - val_pack_accuracy: 0.6377 - val_day_accuracy: 0.4340\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 3.23448\n",
            "Epoch 97/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.3087 - pack_loss: 1.1464 - day_loss: 2.1622 - pack_accuracy: 0.6364 - day_accuracy: 0.4352 - val_loss: 3.2848 - val_pack_loss: 1.1344 - val_day_loss: 2.1504 - val_pack_accuracy: 0.6396 - val_day_accuracy: 0.4377\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 3.23448\n",
            "Epoch 98/100\n",
            "1794568/1794568 [==============================] - 71s 40us/step - loss: 3.2858 - pack_loss: 1.1319 - day_loss: 2.1536 - pack_accuracy: 0.6398 - day_accuracy: 0.4369 - val_loss: 3.3005 - val_pack_loss: 1.1377 - val_day_loss: 2.1628 - val_pack_accuracy: 0.6386 - val_day_accuracy: 0.4338\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 3.23448\n",
            "Epoch 99/100\n",
            "1794568/1794568 [==============================] - 70s 39us/step - loss: 3.2938 - pack_loss: 1.1406 - day_loss: 2.1533 - pack_accuracy: 0.6375 - day_accuracy: 0.4371 - val_loss: 3.2920 - val_pack_loss: 1.1471 - val_day_loss: 2.1449 - val_pack_accuracy: 0.6358 - val_day_accuracy: 0.4386\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 3.23448\n",
            "Epoch 100/100\n",
            "1794568/1794568 [==============================] - 71s 39us/step - loss: 3.2774 - pack_loss: 1.1313 - day_loss: 2.1465 - pack_accuracy: 0.6395 - day_accuracy: 0.4384 - val_loss: 3.2793 - val_pack_loss: 1.1361 - val_day_loss: 2.1433 - val_pack_accuracy: 0.6378 - val_day_accuracy: 0.4386\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 3.23448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9cf224bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhc9xPEPrDqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/My Drive/pack_reco_engine_new_with_rcg_info.h5')\n",
        "\n",
        "p=loaded_model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ANgI3XSXpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "9210c5a4-9ae3-4593-ddce-c039936da216"
      },
      "source": [
        "pd.set_option('display.max_columns', 500)\n",
        "pred=pd.DataFrame(p[0])\n",
        "pred.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.347146</td>\n",
              "      <td>0.161935</td>\n",
              "      <td>0.276634</td>\n",
              "      <td>0.019264</td>\n",
              "      <td>0.079880</td>\n",
              "      <td>0.069738</td>\n",
              "      <td>0.011972</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.010734</td>\n",
              "      <td>0.008876</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.001267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.031442</td>\n",
              "      <td>0.004140</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.027355</td>\n",
              "      <td>0.002337</td>\n",
              "      <td>0.008687</td>\n",
              "      <td>0.005085</td>\n",
              "      <td>0.002028</td>\n",
              "      <td>0.882822</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.008742</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.004049</td>\n",
              "      <td>0.018919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.016540</td>\n",
              "      <td>0.005350</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>0.012780</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.950620</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.003213</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.401096</td>\n",
              "      <td>0.038112</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.002983</td>\n",
              "      <td>0.404609</td>\n",
              "      <td>0.014903</td>\n",
              "      <td>0.029172</td>\n",
              "      <td>0.009623</td>\n",
              "      <td>0.006555</td>\n",
              "      <td>0.055325</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.005729</td>\n",
              "      <td>0.004697</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>0.004727</td>\n",
              "      <td>0.014860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.491298</td>\n",
              "      <td>0.022056</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.004215</td>\n",
              "      <td>0.379231</td>\n",
              "      <td>0.020764</td>\n",
              "      <td>0.023679</td>\n",
              "      <td>0.003710</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>0.025639</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>0.002487</td>\n",
              "      <td>0.005327</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.001799</td>\n",
              "      <td>0.004170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.347146  0.161935  0.276634  0.019264  0.079880  0.069738  0.011972   \n",
              "1  0.031442  0.004140  0.000170  0.000277  0.027355  0.002337  0.008687   \n",
              "2  0.016540  0.005350  0.002333  0.012780  0.002441  0.950620  0.000326   \n",
              "3  0.401096  0.038112  0.002780  0.002983  0.404609  0.014903  0.029172   \n",
              "4  0.491298  0.022056  0.003313  0.004215  0.379231  0.020764  0.023679   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.001433  0.010734  0.008876  0.004852  0.001433  0.003838  0.000318   \n",
              "1  0.005085  0.002028  0.882822  0.000674  0.008742  0.001381  0.001892   \n",
              "2  0.000068  0.004086  0.001015  0.001064  0.000111  0.003213  0.000005   \n",
              "3  0.009623  0.006555  0.055325  0.001425  0.005729  0.004697  0.003404   \n",
              "4  0.003710  0.009469  0.025639  0.001924  0.002487  0.005327  0.000919   \n",
              "\n",
              "         14        15  \n",
              "0  0.000679  0.001267  \n",
              "1  0.004049  0.018919  \n",
              "2  0.000010  0.000038  \n",
              "3  0.004727  0.014860  \n",
              "4  0.001799  0.004170  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIBHRCRbYKsW",
        "colab_type": "text"
      },
      "source": [
        "# Select TOP 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V-TgGUhgHaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top3=np.argsort(-p[0])[:,:3]\n",
        "#pd.DataFrame(top3)\n",
        "ytest=np.argmax(y_test[:,:16],axis=1)\n",
        "\n",
        "val=[]\n",
        "for i in range(y_test.shape[0]):\n",
        "  if ytest[i] in top3[i]:\n",
        "    val.append(1)\n",
        "  else:\n",
        "    val.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsLOXq6icuVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bd8b347a-585e-4882-ad15-7a86fb971cbd"
      },
      "source": [
        "pd.Series(val).value_counts()/(221887+27359)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.890233\n",
              "0    0.109767\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia8oqokDcVLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(val,ytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RALKcZrIrDq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_class=np.argmax(p[0], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCjeGOt6rDrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual=np.argmax(y_test[:,:16],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF8k9FoCrDrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "12279529-75c6-43f4-f3aa-0daaf86f585d"
      },
      "source": [
        "pd.crosstab(pred_class,actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58266</td>\n",
              "      <td>6307</td>\n",
              "      <td>788</td>\n",
              "      <td>1070</td>\n",
              "      <td>7354</td>\n",
              "      <td>4658</td>\n",
              "      <td>1614</td>\n",
              "      <td>347</td>\n",
              "      <td>1837</td>\n",
              "      <td>2955</td>\n",
              "      <td>577</td>\n",
              "      <td>425</td>\n",
              "      <td>2049</td>\n",
              "      <td>384</td>\n",
              "      <td>365</td>\n",
              "      <td>1205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1784</td>\n",
              "      <td>6885</td>\n",
              "      <td>227</td>\n",
              "      <td>121</td>\n",
              "      <td>293</td>\n",
              "      <td>502</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>23</td>\n",
              "      <td>28</td>\n",
              "      <td>5</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>168</td>\n",
              "      <td>139</td>\n",
              "      <td>846</td>\n",
              "      <td>34</td>\n",
              "      <td>33</td>\n",
              "      <td>89</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>987</td>\n",
              "      <td>386</td>\n",
              "      <td>64</td>\n",
              "      <td>3702</td>\n",
              "      <td>141</td>\n",
              "      <td>886</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>27</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7721</td>\n",
              "      <td>1799</td>\n",
              "      <td>249</td>\n",
              "      <td>347</td>\n",
              "      <td>19828</td>\n",
              "      <td>1276</td>\n",
              "      <td>1752</td>\n",
              "      <td>130</td>\n",
              "      <td>332</td>\n",
              "      <td>1405</td>\n",
              "      <td>71</td>\n",
              "      <td>127</td>\n",
              "      <td>185</td>\n",
              "      <td>83</td>\n",
              "      <td>109</td>\n",
              "      <td>307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5643</td>\n",
              "      <td>2761</td>\n",
              "      <td>735</td>\n",
              "      <td>1991</td>\n",
              "      <td>761</td>\n",
              "      <td>30519</td>\n",
              "      <td>164</td>\n",
              "      <td>25</td>\n",
              "      <td>857</td>\n",
              "      <td>196</td>\n",
              "      <td>320</td>\n",
              "      <td>21</td>\n",
              "      <td>531</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>827</td>\n",
              "      <td>219</td>\n",
              "      <td>25</td>\n",
              "      <td>21</td>\n",
              "      <td>859</td>\n",
              "      <td>155</td>\n",
              "      <td>2701</td>\n",
              "      <td>6</td>\n",
              "      <td>57</td>\n",
              "      <td>242</td>\n",
              "      <td>17</td>\n",
              "      <td>94</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>87</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1223</td>\n",
              "      <td>371</td>\n",
              "      <td>307</td>\n",
              "      <td>194</td>\n",
              "      <td>331</td>\n",
              "      <td>1086</td>\n",
              "      <td>72</td>\n",
              "      <td>14</td>\n",
              "      <td>7475</td>\n",
              "      <td>174</td>\n",
              "      <td>281</td>\n",
              "      <td>42</td>\n",
              "      <td>336</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>699</td>\n",
              "      <td>108</td>\n",
              "      <td>10</td>\n",
              "      <td>22</td>\n",
              "      <td>574</td>\n",
              "      <td>123</td>\n",
              "      <td>163</td>\n",
              "      <td>110</td>\n",
              "      <td>61</td>\n",
              "      <td>4235</td>\n",
              "      <td>29</td>\n",
              "      <td>125</td>\n",
              "      <td>54</td>\n",
              "      <td>23</td>\n",
              "      <td>45</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>218</td>\n",
              "      <td>109</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>86</td>\n",
              "      <td>399</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>269</td>\n",
              "      <td>49</td>\n",
              "      <td>2706</td>\n",
              "      <td>8</td>\n",
              "      <td>144</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>123</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>12</td>\n",
              "      <td>25</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>130</td>\n",
              "      <td>4</td>\n",
              "      <td>523</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3661</td>\n",
              "      <td>237</td>\n",
              "      <td>73</td>\n",
              "      <td>199</td>\n",
              "      <td>406</td>\n",
              "      <td>848</td>\n",
              "      <td>97</td>\n",
              "      <td>27</td>\n",
              "      <td>720</td>\n",
              "      <td>336</td>\n",
              "      <td>231</td>\n",
              "      <td>38</td>\n",
              "      <td>12338</td>\n",
              "      <td>13</td>\n",
              "      <td>31</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>358</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>735</td>\n",
              "      <td>77</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>273</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>129</td>\n",
              "      <td>624</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2410</td>\n",
              "      <td>143</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>462</td>\n",
              "      <td>75</td>\n",
              "      <td>65</td>\n",
              "      <td>150</td>\n",
              "      <td>27</td>\n",
              "      <td>531</td>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>55</td>\n",
              "      <td>606</td>\n",
              "      <td>1120</td>\n",
              "      <td>3933</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1    2     3      4      5   ...    10   11     12   13    14    15\n",
              "row_0                                        ...                                   \n",
              "0      58266  6307  788  1070   7354   4658  ...   577  425   2049  384   365  1205\n",
              "1       1784  6885  227   121    293    502  ...    28    5     35    0     0     2\n",
              "2        168   139  846    34     33     89  ...     2    0      4    2     1     0\n",
              "3        987   386   64  3702    141    886  ...    20    0     52    4     1     3\n",
              "4       7721  1799  249   347  19828   1276  ...    71  127    185   83   109   307\n",
              "5       5643  2761  735  1991    761  30519  ...   320   21    531   14    14    32\n",
              "6        827   219   25    21    859    155  ...    17   94     41    2     3    16\n",
              "7         87    11    1     1     39      4  ...     0    0      2    7    11    18\n",
              "8       1223   371  307   194    331   1086  ...   281   42    336    3     8    26\n",
              "9        699   108   10    22    574    123  ...    29  125     54   23    45   171\n",
              "10       218   109   35    64     86    399  ...  2706    8    144    1     1    10\n",
              "11       123    11    0     1     70     12  ...     4  523     12    1     4    10\n",
              "12      3661   237   73   199    406    848  ...   231   38  12338   13    31   168\n",
              "13       358    15    0     1     21      6  ...     1    3      0  735    77   206\n",
              "14       273    18    0     0     52      8  ...     2    4      2  129   624   660\n",
              "15      2410   143   10    10    462     75  ...     4   44     55  606  1120  3933\n",
              "\n",
              "[16 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbn1L9oGrDrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "02a3bec7-960f-4a68-ac03-75c944339519"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val,ytest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.04      0.02     28292\n",
            "           1       0.79      0.07      0.13    221708\n",
            "           2       0.00      0.00      0.00         0\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.00      0.00      0.00         0\n",
            "           7       0.00      0.00      0.00         0\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         0\n",
            "          11       0.00      0.00      0.00         0\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.00      0.00      0.00         0\n",
            "          14       0.00      0.00      0.00         0\n",
            "          15       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.07    250000\n",
            "   macro avg       0.05      0.01      0.01    250000\n",
            "weighted avg       0.70      0.07      0.12    250000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO-HhW1so9fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "de83159e-c2ba-45ca-c606-9a0092be5488"
      },
      "source": [
        "act=np.argmax(y_test[:,:16],axis=1)\n",
        "pd.Series(act).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     84289\n",
              "5     40686\n",
              "4     30867\n",
              "1     19502\n",
              "12    15828\n",
              "8     11855\n",
              "9     10503\n",
              "3      7758\n",
              "6      6803\n",
              "15     6774\n",
              "10     4267\n",
              "2      3240\n",
              "14     2469\n",
              "13     1962\n",
              "11     1332\n",
              "7      1111\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfNzChWJrDrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5b70b845-4131-4d9b-b2b3-1b0392f0b6b1"
      },
      "source": [
        "pd.Series(pred_class).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     102662\n",
              "5      35782\n",
              "4      29786\n",
              "12     15693\n",
              "1      14401\n",
              "8      12617\n",
              "15      9488\n",
              "9       8658\n",
              "3       6649\n",
              "6       5386\n",
              "10      3250\n",
              "2       1567\n",
              "13      1526\n",
              "11      1094\n",
              "7        573\n",
              "14       114\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}